{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f308b1",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression Vs Simple Linear Regression\n",
    "\n",
    "Multiple Linear Regression and Simple Linear Regression are both techniques used in the field of regression analysis to model the relationship between independent and dependent variables. However, they differ in terms of the number of independent variables they consider and the complexity of their models. Here's a comparison between the two:\n",
    "\n",
    "1. **Number of Independent Variables:**\n",
    "   - **Simple Linear Regression:** This method involves only one independent variable ($\\texttt(x$)) and one dependent variable ($\\texttt(y$)).\n",
    "   - **Multiple Linear Regression:** In this approach, there are multiple independent variables ($\\texttt(x_1, x_2, \\ldots, x_n$)) and one dependent variable ($\\texttt(y$)).\n",
    "\n",
    "2. **Equation:**\n",
    "   - **Simple Linear Regression:** The equation is a basic line equation: $\\texttt( y = b_0 + b_1x + \\varepsilon $), where $\\texttt(b_0$) is the intercept, $\\texttt(b_1$) is the coefficient, $\\texttt(x$) is the independent variable, and $\\texttt(\\varepsilon$) represents the error term.\n",
    "   - **Multiple Linear Regression:** The equation becomes more complex as it accounts for multiple independent variables: $\\texttt( y = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n + \\varepsilon $).\n",
    "\n",
    "3. **Purpose:**\n",
    "   - **Simple Linear Regression:** Useful when you want to understand the linear relationship between two variables and predict one variable based on another.\n",
    "   - **Multiple Linear Regression:** Applicable when you have multiple independent variables that may collectively affect the dependent variable. It helps in identifying the influence of each independent variable while considering the others.\n",
    "\n",
    "4. **Interpretation:**\n",
    "   - **Simple Linear Regression:** The relationship between the variables is straightforward and easy to interpret. The slope represents the change in the dependent variable for a unit change in the independent variable.\n",
    "   - **Multiple Linear Regression:** Interpretation becomes more complex since each coefficient ($\\texttt(b_1, b_2, \\ldots, b_n$)) represents the change in the dependent variable while holding other variables constant. It helps identify the individual impact of each variable.\n",
    "\n",
    "5. **Model Complexity:**\n",
    "   - **Simple Linear Regression:** Simpler model with fewer parameters to estimate.\n",
    "   - **Multiple Linear Regression:** More complex model with multiple parameters to estimate.\n",
    "\n",
    "6. **Applications:**\n",
    "   - **Simple Linear Regression:** Commonly used for basic predictive modeling, such as predicting a student's score based on the number of hours studied.\n",
    "   - **Multiple Linear Regression:** Suitable for scenarios where multiple factors influence the outcome, such as predicting a house's price based on features like area, number of rooms, and location.\n",
    "\n",
    "7. **Assumptions:**\n",
    "   - Both techniques assume a linear relationship between variables, independence of errors, and constant variance of errors.\n",
    "\n",
    "In summary, Simple Linear Regression is appropriate when dealing with only two variables and a straightforward relationship, while Multiple Linear Regression is more suited for cases involving multiple independent variables and more complex relationships. Each technique serves different analytical needs and provides insights into different aspects of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c2896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81e36f8b",
   "metadata": {},
   "source": [
    "# Key formulas used in Simple Linear Regression:\n",
    "\n",
    "1. **Simple Linear Regression Equation:**\n",
    "   The equation models the relationship between an independent variable ($\\texttt(x$)) and a dependent variable ($\\texttt(y$)):\n",
    "\n",
    "   $\\texttt y = mx + b $\n",
    "\n",
    "   where:\n",
    "   - $\\texttt(y$) is the dependent variable (response).\n",
    "   - $\\texttt(x$) is the independent variable (predictor).\n",
    "   - $\\texttt(m$) is the slope of the regression line.\n",
    "   - $\\texttt(b$) is the intercept (y-intercept) of theregression line.\n",
    "\n",
    "2. **Slope ($\\texttt(m$)) Calculation:**\n",
    "   The slope ($\\texttt(m$)) of the regression line is calculated using the following formula:\n",
    "\n",
    "   $\\texttt m = \\frac{n(\\sum_{i=1}^{n} x_iy_i) - (\\sum_{i=1}^{n} x_i)(\\sum_{i=1}^{n} y_i)}{n(\\sum_{i=1}^{n} x_i^2) - (\\sum_{i=1}^{n} x_i)^2} $\n",
    "\n",
    "   where:\n",
    "   - $\\texttt(n$) is the number of data points.\n",
    "   - $\\texttt(x_i$) is the $\\texttt($\\)th value of the independent variable.\n",
    "   - $\\texttt(y_i$) is the $\\texttt(i$)th value of the dependent variable.\n",
    "\n",
    "3. **Intercept ($\\texttt(b$)) Calculation:**\n",
    "   The intercept ($\\texttt(b$)) of the regression line is calculated using the following formula:\n",
    "\n",
    "   $\\texttt b = \\frac{\\sum_{i=1}^{n} y_i - m\\sum_{i=1}^{n} x_i}{n} $\n",
    "\n",
    "4. **Predictions:**\n",
    "   Predictions for the dependent variable ($\\texttt(y$)) can be made using the linear equation:\n",
    "\n",
    "   $\\texttt (\\hat{y} = mx + b $)\n",
    "\n",
    "   where:\n",
    "   - $\\texttt(\\hat{y}$) is the predicted value of the dependent variable.\n",
    "   - $\\texttt(x$) is the value of the independent variable.\n",
    "\n",
    "5. **Residuals and Sum of Squared Residuals (SSR):**\n",
    "   Residuals ($\\texttt(e_i$)) represent the differences between the actual ($\\texttt(y_i$)) and predicted ($\\texttt(\\hat{y_i}$)) values. The sum of squared residuals (SSR) is calculated as:\n",
    "\n",
    "   $\\texttt SSR = \\sum_{i=1}^{n} e_i^2 $\n",
    "\n",
    "6. **Coefficient of Determination ($\\texttt(R^2$)):**\n",
    "   The coefficient of determination ($\\texttt(R^2$)) indicates the proportion of variance in the dependent variable explained by the independent variable. It is calculated as:\n",
    "\n",
    "   $\\texttt R^2 = 1 - \\frac{SSR}{SST} $\n",
    "\n",
    "   where SST is the total sum of squares.\n",
    "\n",
    "These formulas are fundamental to Simple Linear Regression and are used to model and analyze the relationship between two variables, make predictions, and evaluate the model's goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8059e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12981713",
   "metadata": {},
   "source": [
    "# Key formulas used in Multiple Linear Regression:\n",
    "\n",
    "1. **Multiple Linear Regression Equation:**\n",
    "   The equation models the relationship between multiple independent variables ($\\texttt(x_1, x_2, \\ldots, x_n$)) and a dependent variable ($\\texttt(y$)):\n",
    "\n",
    "  $\\texttt  y = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n + \\varepsilon $\n",
    "\n",
    "   where:\n",
    "   - $\\texttt(y$) is the dependent variable (response).\n",
    "   - $\\texttt(x_1, x_2, \\ldots, x_n$) are independent variables (predictors).\n",
    "   - $\\texttt(b_0$) is the intercept (y-intercept).\n",
    "   - $\\texttt(b_1, b_2, \\ldots, b_n$) are coefficients corresponding to each independent variable.\n",
    "   - $\\texttt(\\varepsilon$) is the error term.\n",
    "\n",
    "2. **Coefficients Calculation:**\n",
    "   The coefficients (\\(b_0, b_1, \\ldots, b_n\\)) are calculated using the Ordinary Least Squares (OLS) method to minimize the sum of squared differences between the predicted values and actual values:\n",
    "\n",
    "\n",
    "\n",
    "   $\\texttt b_j = \\frac{\\sum_{i=1}^{n} (x_{ij} - \\bar{x_j})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_{ij} - \\bar{x_j})^2} $\n",
    "\n",
    "   where:\n",
    "   -  $\\texttt(n)$ is the number of data points.\n",
    "   - $\\texttt(x_{ij}$) is the value of the \\(j\\)th independent variable for the \\(i\\)th data point.\n",
    "   - $\\texttt(y_i$) is the value of the dependent variable for the \\(i\\)th data point.\n",
    "   - $\\texttt(\\bar{x_j}$) is the mean of the \\(j\\)th independent variable.\n",
    "   - $\\texttt(\\bar{y}$) is the mean of the dependent variable.\n",
    "\n",
    "3. **Model Evaluation:**\n",
    "   The performance of the model can be assessed using metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared (\\( R^2 \\)):\n",
    "\n",
    "  $\\texttt MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 $\n",
    "   \n",
    "   $\\texttt RMSE = \\sqrt{MSE} $\n",
    "   \n",
    "   $\\texttt R^2 = 1 - \\frac{SSR}{SST} $\n",
    "   \n",
    "   where:\n",
    "   - $\\texttt(y_i$) is the actual value of the dependent variable for the \\(i\\)th data point.\n",
    "   - $\\texttt( \\hat{y_i} $) is the predicted value of the dependent variable for the \\(i\\)th data point.\n",
    "   - $\\texttt(SSR$) is the sum of squared residuals.\n",
    "   - $\\texttt(SST$) is the total sum of squares.\n",
    "\n",
    "4. **Feature Scaling:**\n",
    "   Feature scaling is not necessary for Multiple Linear Regression since the coefficients account for the scales of the variables. However, if regularization methods like Ridge or Lasso Regression are used, feature scaling may be beneficial.\n",
    "\n",
    "These formulas form the foundation of Multiple Linear Regression, allowing you to analyze the relationships between multiple variables and make predictions based on the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319cc154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e662c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
